---
title: "Lab 08 STATS 369"
author: "Anish Hota"
date: "2025-10-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r cache=TRUE}
library(xgboost)
library(readr)
library(DiagrammeR)
data(agaricus.train, package='xgboost')
data(agaricus.test, package='xgboost')


bst <- xgboost(data = agaricus.train$data, label = agaricus.train$label, 
               max_depth = 2, eta = 1, nthread = 2, nrounds = 2, 
               objective = "binary:logistic")
pred <- predict(bst, agaricus.test$data)
```

## Q1

max_depth = 2 : is the maximum depth of each decision tree, which is set to 2.

eta = 1: is the learning rate when =1 it means no shrinkage.

nrounds = 2: Number of trees fitted, in this case 2.

## Q2

```{r cache=TRUE}
xgb.plot.tree(model = bst, trees = 0)
xgb.plot.tree(model = bst, trees = 1)
gv <- xgb.plot.tree(model = bst, trees = 0, render = FALSE)
```

## Q3

```{r cache=TRUE}
dtrain <- xgb.DMatrix(data = agaricus.train$data, label = agaricus.train$label)
dtest  <- xgb.DMatrix(data = agaricus.test$data,  label = agaricus.test$label)
cv <- xgb.cv(
  params = list(objective = "binary:logistic", max_depth = 2, eta = 1),
  data = dtrain,
  nrounds = 200,
  nfold = 5,
  metrics = list("logloss","error"),
  early_stopping_rounds = 10,
  verbose = 1,
  showsd = TRUE,
  stratified = TRUE
)

best_nrounds <- cv$best_iteration
best_nrounds

bst_cv <- xgboost(
  data = agaricus.train$data,
  label = agaricus.train$label,
  max_depth = 2,
  eta = 1,
  nrounds = best_nrounds,
  objective = "binary:logistic",
  verbose = 1
)

pred_orig <- predict(bst, agaricus.test$data)      # original lab model (nrounds=2)
pred_cv   <- predict(bst_cv, agaricus.test$data)   # cv-chosen model

err_orig <- mean((pred_orig > 0.5) != agaricus.test$label)
err_cv   <- mean((pred_cv   > 0.5) != agaricus.test$label)

cat(sprintf("Test error - original (nrounds=2): %.4f\n", err_orig))
cat(sprintf("Test error - CV model (nrounds=%d): %.4f\n", best_nrounds, err_cv))
```
best_nrounds is much larger than 2, so the CV model is more complex and more accurate on test data.

## Q4

```{r cache=TRUE}
raw_lines <- read_lines("mushroom.test")

split_lines <- lapply(strsplit(raw_lines, "\\s+"), function(x) x[x != ""])

mush_raw <- as.data.frame(do.call(rbind, split_lines), stringsAsFactors = FALSE)

attrs <- mush_raw[-1, 1]
mush_data <- mush_raw[-1, -1]
colnames(mush_data) <- mush_raw[1, -1]
rownames(mush_data) <- attrs

mush_test <- as.data.frame(t(mush_data), stringsAsFactors = TRUE)

mush_test[] <- lapply(mush_test, as.factor)

train_colnames <- dimnames(agaricus.train$data)[[2]]
mm_test <- model.matrix(~ . - 1, data = mush_test)

mm_test_df <- as.data.frame(mm_test)
for (col in setdiff(train_colnames, colnames(mm_test_df))) {
  mm_test_df[[col]] <- 0
}
mm_test_df <- mm_test_df[, train_colnames]
mm_test_aligned <- as.matrix(mm_test_df)

pred_probs <- predict(bst, mm_test_aligned)
pred_class <- ifelse(pred_probs > 0.5, 1, 0)

results <- data.frame(
  Mushroom = colnames(mush_data),   # A, B, C
  Prob_Edible = round(pred_probs, 3),
  Pred_Class = ifelse(pred_class == 1, "edible", "poisonous")
)

print(results)
```

## Q5

Amanita phalloides — Death cap

Amanita virosa — Destroying angel

Volvariella volvacea — Straw mushroom

## Citatons and Acknowledgements

I used ChatGPT to help out with question 4.